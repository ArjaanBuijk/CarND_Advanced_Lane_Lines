{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Advanced Finding of Lane Lines on the Road** \n",
    "***\n",
    "In this project, I am applying the tools learned to identify lane lines on the road. My approach was to take the Jupyter notebook of Project 1, which did a decent job already on finding the lanes, and expand it with the new techniques learned in Lesson 16. As in Project 1, I first developed the pipeline on a series of individual images, and later apply the result to the provided video streams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Camera calibration\n",
    "2. Distortion correction\n",
    "3. Color & Gradient threshold\n",
    "4. Perspective transform\n",
    "5. Detect lane lines\n",
    "6. Determine the lane curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from skimage import exposure\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some global utility functions, mainly for visualization\n",
    "\n",
    "\n",
    "# function to show an image with title\n",
    "def show_image(image, title, cmap=None ):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(24, 9))\n",
    "    plt.title(title)\n",
    "    if cmap:\n",
    "        plt.imshow(image, cmap=cmap) # if you wanted to show a single color channel image called 'gray', for example, call as plt.imshow(gray, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(image)  \n",
    "    plt.show()\n",
    "    \n",
    "# function to plot original & modifed images side-by-side\n",
    "def plot_orig_and_changed_image(image1, description1='Original Image',\n",
    "                                image2=None, description2='Changed Image',\n",
    "                                file_out=None):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    if image1.ndim == 3 and image1.shape[2] ==3:\n",
    "        ax1.imshow(image1)\n",
    "    else:\n",
    "        ax1.imshow(image1.squeeze(), cmap='gray')\n",
    "    ax1.set_title(description1, fontsize=50)\n",
    "    \n",
    "    if image2 is not None:\n",
    "        if image2.ndim == 3 and image2.shape[2] == 3:\n",
    "            ax2.imshow(image2)\n",
    "        else:\n",
    "            ax2.imshow(image2.squeeze(), cmap='gray')\n",
    "        ax2.imshow(image2, cmap='gray')\n",
    "        ax2.set_title(description2, fontsize=50)\n",
    "    \n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    plt.show()\n",
    "    \n",
    "    if file_out is not None:\n",
    "        f.savefig(file_out)\n",
    "        print('Writen file: '+file_out)\n",
    "        \n",
    "    plt.close(f)\n",
    "\n",
    "# function to draw straight lines on top of an rgb image\n",
    "def draw_lines_on_image(img, lines, color=[255, 0, 0], thickness=2):\n",
    "    \"\"\"\n",
    "    This function draws `lines` with `color` and `thickness` on an rgb image.    \n",
    "    Lines are drawn on the image inplace (mutates the image).\n",
    "    \"\"\"\n",
    "    for line in lines:\n",
    "        p1 = line[0]\n",
    "        p2 = line[1]\n",
    "        cv2.line(img, p1, p2, color, thickness)\n",
    "        \n",
    "# function to draw the outline of square windows on top of an rgb image\n",
    "def draw_windows_on_image(img, \n",
    "                          window_centroids, window_width, window_height, \n",
    "                          color=[0, 255, 0]):\n",
    "    dw = 0.5*window_width\n",
    "    dh = 0.5*window_height \n",
    "    yc = img.shape[0]-dh   # y at center of window\n",
    "    \n",
    "    # store the outlines of each window\n",
    "    lines = []\n",
    "    for centroids in window_centroids: # levels of windows\n",
    "        for xc in centroids:  # x at center of left & right windows at each level\n",
    "            p1 = (int(xc+dw), int(yc+dh)) # top right      - make it all integer, because draw_lines needs pixel #s.\n",
    "            p2 = (int(xc+dw), int(yc-dh)) # bottom right\n",
    "            p3 = (int(xc-dw), int(yc-dh)) # bottom left\n",
    "            p4 = (int(xc-dw), int(yc+dh)) # top left\n",
    "            lines.append((p1,p2))\n",
    "            lines.append((p2,p3))\n",
    "            lines.append((p3,p4))\n",
    "            lines.append((p4,p1))\n",
    "            \n",
    "        yc = yc - window_height    \n",
    "            \n",
    "    # draw lines on the image in green\n",
    "    draw_lines_on_image(img, lines, color=[0, 255, 0], thickness=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Camera Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare object points\n",
    "nx = 9 # The number of inside corners in x\n",
    "ny = 6 # The number of inside corners in y\n",
    "\n",
    "# Make a list of calibration images\n",
    "file_dir = \"camera_cal\"\n",
    "file_dir_out = \"camera_cal_output\"\n",
    "files = os.listdir(file_dir)\n",
    "\n",
    "# termination criteria\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((ny*nx,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1,2) # x, y coordinates\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "print('Finding chessboard in calibration images')\n",
    "for file in files:\n",
    "    file_in=file_dir+\"/\"+file\n",
    "    file_out=file_dir_out+\"/Corners_on_\"+file\n",
    "    \n",
    "    image = mpimg.imread(file_in)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "    # If found\n",
    "    # - draw corners\n",
    "    # - append object points & image points to storage arrays\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)    # note: same for all calibration images !\n",
    "        imgpoints.append(corners)\n",
    "        \n",
    "        # Draw and display the corners\n",
    "        image2 = np.copy(image)\n",
    "        cv2.drawChessboardCorners(image2, (nx, ny), corners, ret)\n",
    "        plot_orig_and_changed_image(image1=image , description1=file,\n",
    "                                    image2=image2, description2='With Corners',\n",
    "                                    file_out=file_out)\n",
    "    else:\n",
    "        plot_orig_and_changed_image(image1=image, description1=file,\n",
    "                                    image2=None , description2=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Calibrating the Camera')\n",
    "# Calibrate the camera\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, \n",
    "                                                   imgpoints, \n",
    "                                                   gray.shape[::-1], \n",
    "                                                   None, None)\n",
    "\n",
    "# Save the calibrated camera data\n",
    "pickle_file = 'calibrated_camera.pickle'\n",
    "\n",
    "try:\n",
    "    with open(pickle_file, 'wb') as pfile:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                'ret': ret,\n",
    "                'mtx': mtx,\n",
    "                'dist': dist,\n",
    "                'rvecs': rvecs,\n",
    "                'tvecs': tvecs,\n",
    "            },\n",
    "            pfile, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "    \n",
    "print('Cached calibrated camera data in pickle file: '+pickle_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Distortion Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'calibrated_camera.pickle'\n",
    "print('Reading calibration data from pickle file: '+pickle_file)\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    pickle_data    = pickle.load(f)\n",
    "    ret    = pickle_data['ret']\n",
    "    mtx    = pickle_data['mtx']\n",
    "    dist   = pickle_data['dist']\n",
    "    rvecs  = pickle_data['rvecs']\n",
    "    tvecs  = pickle_data['tvecs']\n",
    "    del pickle_data  # Free up memory\n",
    "\n",
    "def undistort(image):\n",
    "    return cv2.undistort(image, mtx, dist, None, mtx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Testing the Calibrated Camera')\n",
    "\n",
    "# Make a list of test images\n",
    "file_dir = \"camera_cal\"\n",
    "file_dir_out = \"camera_cal_output\"\n",
    "files = os.listdir(file_dir)\n",
    "\n",
    "for file in files:\n",
    "    file_in=file_dir+\"/\"+file\n",
    "    file_out=file_dir_out+\"/Distortion_Correction_\"+file\n",
    "    \n",
    "    image = mpimg.imread(file_in)\n",
    "    \n",
    "    dst = undistort(image)\n",
    "    \n",
    "    plot_orig_and_changed_image(image1=image , description1=file,\n",
    "                               image2=dst, description2='Distortion Corrected',\n",
    "                               file_out=file_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3. Color & Gradient threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to convert RGB image to grayscale\n",
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    (assuming your grayscaled image is called 'gray')\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
    "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# function to Adaptive Gaussian Thresholding\n",
    "# See - http://docs.opencv.org/trunk/d7/d4d/tutorial_py_thresholding.html\n",
    "def adaptive_gaussian_threshold(gray, thresholds=(0, 255)):\n",
    "    # Otsu's thresholding after Gaussian filtering\n",
    "    #blur = cv2.GaussianBlur(gray,(3,3),0)\n",
    "    #blur = cv2.GaussianBlur(gray,(5,5),0)\n",
    "    blur = cv2.GaussianBlur(gray,(7,7),0)\n",
    "    #blur = cv2.GaussianBlur(gray,(9,9),0)\n",
    "    #blur = cv2.GaussianBlur(gray,(15,15),0)\n",
    "    blur = np.array(blur, dtype=np.uint8) # else it sometimes crashes...\n",
    "    ret3,th3 = cv2.threshold(blur,thresholds[0],thresholds[1],cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    return th3\n",
    "\n",
    "# function to turn off pixels outside crop-window\n",
    "def crop_binary_image(img,crop_left=None, crop_right=None,\n",
    "                          crop_bot=None, crop_top=None):\n",
    "    if crop_left is None:   crop_left=0\n",
    "    if crop_right is None:  crop_right=img.shape[1]\n",
    "    if crop_bot is None:    crop_bot=im.shape[0]\n",
    "    if crop_top is None:    crop_top=0\n",
    "    \n",
    "    image = np.copy(img)\n",
    "    \n",
    "    image[:,0:crop_left] = 0\n",
    "    image[:,crop_right:] = 0\n",
    "    image[0:crop_top,:]  = 0\n",
    "    image[crop_bot:,:]   = 0\n",
    "    \n",
    "    return image\n",
    "\n",
    "# function to convert binary image to RGB\n",
    "def binary_to_rgb(binary_img):\n",
    "    # Create an new rgb image\n",
    "    # binary_image is 2D array with either 0 or 1 values\n",
    "    # --> Where value=1, [R,G,B] = [255, 255, 255] (colors those new image pixels white)\n",
    "    rgb_img = np.stack((binary_img, binary_img, binary_img),axis=-1)*255\n",
    "    return rgb_img\n",
    "    \n",
    "    \n",
    "def abs_sobel_thresh(img, orient='x', ksize=5, thresh=(0,255), rgb_in=True ):\n",
    "    # Convert to grayscale\n",
    "    if rgb_in:\n",
    "        gray = grayscale(img)\n",
    "    else:\n",
    "        gray = img\n",
    "    \n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0,ksize=ksize))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1,ksize=ksize))\n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    # Return the result\n",
    "    return binary_output\n",
    "\n",
    "def mag_thresh(img, ksize=5, mag_thresh=(0, 255), rgb_in=True):\n",
    "    # Convert to grayscale\n",
    "    if rgb_in:\n",
    "        gray = grayscale(img)\n",
    "    else:\n",
    "        gray = img\n",
    "        \n",
    "    # Take both Sobel x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=ksize)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=ksize)\n",
    "    # Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8) \n",
    "    # Create a binary image of ones where threshold is met, zeros otherwise\n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    binary_output[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output\n",
    "\n",
    "def dir_threshold(img, ksize=5, thresh=(0, np.pi/2), rgb_in=True):\n",
    "    # SEE: https://goo.gl/nXmgN0 \n",
    "    # Grayscale\n",
    "    # Note: Make sure you use the correct grayscale conversion depending on how you've read\n",
    "    #       in your images. \n",
    "    #       Use cv2.COLOR_RGB2GRAY if you've read in an image using mpimg.imread(). \n",
    "    #       Use cv2.COLOR_BGR2GRAY if you've read in an image using cv2.imread().\n",
    "    if rgb_in:\n",
    "        gray = grayscale(img)\n",
    "    else:\n",
    "        gray = img\n",
    "        \n",
    "    # Calculate the x and y gradients\n",
    "    # x direction (the 1, 0 at the end denotes x direction)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=ksize)\n",
    "    # y direction (the 0, 1 at the end denotes y direction)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=ksize)\n",
    "    # Take the absolute value of the gradient direction, \n",
    "    # apply a threshold, and create a binary image result\n",
    "    # -> pixels have a value of 1 or 0, based on the strength of the gradient.\n",
    "    absgraddir = np.arctan2(np.absolute(sobely), np.absolute(sobelx))\n",
    "    binary_output =  np.zeros_like(absgraddir)\n",
    "    binary_output[(absgraddir >= thresh[0]) & (absgraddir <= thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output\n",
    "        \n",
    "def pipeline(img, show_all=False):\n",
    "    img = np.copy(img)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Combine some gradient thresholds\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 5 # Choose a larger odd number to smooth gradient measurements\n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(img, orient='x', ksize=ksize, thresh=(20, 100))\n",
    "    grady = abs_sobel_thresh(img, orient='y', ksize=ksize, thresh=(20, 100))\n",
    "    mag_binary = mag_thresh(img, ksize=ksize, mag_thresh=(30, 100))\n",
    "    # Note: threshold is angle: 0 = horizontal; +/- np.pi/2 = vertical\n",
    "    dir_binary = dir_threshold(img, ksize=15, thresh=(0.7, 1.3))\n",
    "\n",
    "    if show_all:\n",
    "        show_image(gradx,      title='Thresholded X-Gradient', cmap='gray')\n",
    "        show_image(grady,      title='Thresholded Y-Gradient', cmap='gray')\n",
    "        show_image(mag_binary, title='Thresholded Magnitude' , cmap='gray')\n",
    "        show_image(dir_binary, title='Thresholded Grad. Dir.', cmap='gray')\n",
    "\n",
    "    # apply Adaptive Gaussian threshold to all of them\n",
    "    gradx = adaptive_gaussian_threshold(gradx, thresholds=(0, 1))\n",
    "    grady = adaptive_gaussian_threshold(grady, thresholds=(0, 1))\n",
    "    mag_binary = adaptive_gaussian_threshold(mag_binary, thresholds=(0, 1))\n",
    "    dir_binary = adaptive_gaussian_threshold(dir_binary, thresholds=(0, 1))\n",
    "\n",
    "    # Combined thresholds\n",
    "    # \n",
    "    # For example, here is a selection for pixels where both the x and y gradients \n",
    "    # meet the threshold criteria, or the gradient magnitude and direction are both \n",
    "    # within their threshold values.\n",
    "\n",
    "    grad_binary = np.zeros_like(gradx)\n",
    "    grad_binary[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "    \n",
    "    if show_all:\n",
    "        show_image(gradx,       title='AGT X-Gradient' , cmap='gray')\n",
    "        show_image(grady,       title='AGT Y-Gradient' , cmap='gray')\n",
    "        show_image(mag_binary,  title='AGT Magnitude'  , cmap='gray')\n",
    "        show_image(dir_binary,  title='AGT Grad. Dir.' , cmap='gray')\n",
    "        show_image(grad_binary, title='AGT Grad Binary', cmap='gray')\n",
    "            \n",
    "        \n",
    "    # ============================================================================\n",
    "    # Threshold the HSV space\n",
    "    \n",
    "    #OK-FOR-PROJECT-VIDEO v_thresh=(225, 255)\n",
    "    h_thresh=(0  , 90)\n",
    "    s_thresh=(100, 255)\n",
    "    v_thresh=(175, 255)\n",
    "    \n",
    "    \n",
    "    # Convert to HSV color space and separate the S channel\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV).astype(np.float)\n",
    "    h_channel = hsv[:,:,0]   # [0-179]\n",
    "    s_channel = hsv[:,:,1]\n",
    "    v_channel = hsv[:,:,2]\n",
    "    \n",
    "    #print ('h_max={0}'.format(np.amax(h_channel)))\n",
    "    #print ('s_max={0}'.format(np.amax(s_channel)))\n",
    "    #print ('v_max={0}'.format(np.amax(v_channel)))\n",
    "    # Threshold color channels\n",
    "    h_binary = np.zeros_like(h_channel)\n",
    "    h_binary[(h_channel >= h_thresh[0]) & (h_channel <= h_thresh[1])] = 1\n",
    "    \n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    \n",
    "    v_binary = np.zeros_like(v_channel)\n",
    "    v_binary[(v_channel >= v_thresh[0]) & (v_channel <= v_thresh[1])] = 1\n",
    "    \n",
    "    hsv_binary = np.zeros_like(h_channel)\n",
    "    hsv_binary[((h_binary == 1) & (s_binary == 1)) & (v_binary == 1)] = 1\n",
    "    \n",
    "    if show_all:\n",
    "        show_image(h_channel,  title='h_hsv_channel', cmap='gray')\n",
    "        show_image(s_channel,  title='s_hsv_channel', cmap='gray')\n",
    "        show_image(v_channel,  title='v_hsv_channel', cmap='gray')\n",
    "        show_image(h_binary,   title='Thresholded h channel', cmap='gray')\n",
    "        show_image(s_binary,   title='Thresholded s channel', cmap='gray')\n",
    "        show_image(v_binary,   title='Thresholded v channel', cmap='gray')\n",
    "        show_image(hsv_binary, title='Thresholded hsv channel', cmap='gray')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Threshold HLS space\n",
    "    \n",
    "    h_thresh=(0  , 90)\n",
    "    l_thresh=(175, 255)\n",
    "    s_thresh=(100, 255)\n",
    "    \n",
    "    # Convert to HLS color space and separate the S channel\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS).astype(np.float)\n",
    "    h_channel = hls[:,:,0]   # [0-179]\n",
    "    l_channel = hls[:,:,1]\n",
    "    s_channel = hls[:,:,2]\n",
    "    \n",
    "    #print ('h_max={0}'.format(np.amax(h_channel)))\n",
    "    #print ('l_max={0}'.format(np.amax(l_channel)))\n",
    "    #print ('s_max={0}'.format(np.amax(s_channel)))\n",
    "    \n",
    "    # Threshold color channels\n",
    "    h_binary = np.zeros_like(h_channel)\n",
    "    h_binary[(h_channel >= h_thresh[0]) & (h_channel <= h_thresh[1])] = 1\n",
    "    \n",
    "    l_binary = np.zeros_like(l_channel)\n",
    "    l_binary[(l_channel >= l_thresh[0]) & (l_channel <= l_thresh[1])] = 1\n",
    "    \n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    \n",
    "    hls_binary = np.zeros_like(h_channel)\n",
    "    hls_binary[((h_binary == 1) & (s_binary == 1)) & (l_binary == 1)] = 1\n",
    "    \n",
    "    if show_all:\n",
    "        show_image(h_channel,  title='h_hls_channel', cmap='gray')\n",
    "        show_image(l_channel,  title='l_hls_channel', cmap='gray')\n",
    "        show_image(s_channel,  title='s_hls_channel', cmap='gray')\n",
    "        show_image(h_binary,   title='Thresholded h channel', cmap='gray')\n",
    "        show_image(l_binary,   title='Thresholded l channel', cmap='gray')\n",
    "        show_image(s_binary,   title='Thresholded s channel', cmap='gray')\n",
    "        show_image(hls_binary, title='Thresholded hls channel', cmap='gray')\n",
    "        \n",
    "    # ============================================================================d\n",
    "    # Stack each channel\n",
    "    color_binary = np.dstack((grad_binary, hsv_binary, hls_binary))\n",
    "    \n",
    "    # Combine the binary thresholds\n",
    "    combined_binary = np.zeros_like(grad_binary)\n",
    "    combined_binary[(grad_binary == 1) | (hsv_binary == 1) | (hls_binary == 1) ] = 1  \n",
    "    \n",
    "    return color_binary, combined_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Testing the color & gradient thresholding')\n",
    "\n",
    "# Make a list of test images\n",
    "file_dir = \"test_images\"\n",
    "file_dir_out = \"test_images_output\"\n",
    "files = os.listdir(file_dir)\n",
    "\n",
    "#files = ['project_video_output0994.jpg',\n",
    "#         'challenge_video_output0001.jpg',\n",
    "#         'challenge_video_output0239.jpg']\n",
    "\n",
    "#files = ['challenge_video_output0239.jpg']\n",
    "\n",
    "show_all=False\n",
    "for file in files:\n",
    "    file_in=file_dir+\"/\"+file\n",
    "    file_out1=file_dir_out+\"/color_binary_\"+file\n",
    "    file_out2=file_dir_out+\"/combined_binary_\"+file\n",
    "    \n",
    "    image = mpimg.imread(file_in)\n",
    "    \n",
    "    # distortion correct the image\n",
    "    image = undistort(image)\n",
    "    \n",
    "    if show_all:\n",
    "        show_image(image, title=file)\n",
    "    \n",
    "    # threshold the image\n",
    "    color_binary, combined_binary = pipeline(image, show_all=show_all)\n",
    "    \n",
    "    plot_orig_and_changed_image(image1=image , description1=file,\n",
    "                               image2=color_binary, description2='color binary',\n",
    "                               file_out=file_out1)\n",
    "    \n",
    "    plot_orig_and_changed_image(image1=image , description1=file,\n",
    "                               image2=combined_binary, description2='thresholded binary',\n",
    "                               file_out=file_out2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Perspective transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interactively find the 4 corner points that will be used as the source points\n",
    "file_in=\"test_images/straight_lines1.jpg\"   \n",
    "img = mpimg.imread(file_in)\n",
    "\n",
    "image = np.copy(img)\n",
    "# distortion correct the image\n",
    "image = undistort(image)\n",
    "\n",
    "# source points\n",
    "p1 = ( 723,            475) # top right\n",
    "p2 = (1110, image.shape[0]) # bottom right\n",
    "p3 = ( 204, image.shape[0]) # bottom left\n",
    "p4 = ( 562,            475) # top left\n",
    "\n",
    "#p1 = ( 685,            450) # top right\n",
    "#p2 = (1110, image.shape[0]) # bottom right\n",
    "#p3 = ( 204, image.shape[0]) # bottom left\n",
    "#p4 = ( 597,            450) # top left\n",
    "\n",
    "# draw source lines on the image in red\n",
    "src_lines = [[p1,p2], [p2,p3], [p3,p4], [p4,p1]]\n",
    "draw_lines_on_image(image, src_lines, color=[255, 0, 0], thickness=2)\n",
    "\n",
    "# destination points\n",
    "correctX = 100\n",
    "dp1 = ( p2[0]-correctX,              0) # top right\n",
    "dp2 = ( p2[0]-correctX, image.shape[0]) # bottom right\n",
    "dp3 = ( p3[0]+correctX, image.shape[0]) # bottom left\n",
    "dp4 = ( p3[0]+correctX,              0) # top left\n",
    "\n",
    "# draw destination lines on the image in green\n",
    "dst_lines = [[dp1,dp2], [dp2,dp3], [dp3,dp4], [dp4,dp1]]\n",
    "draw_lines_on_image(image, dst_lines, color=[0, 255, 0], thickness=2)\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = fig.gca()\n",
    "ax.set_xticks(np.arange(0, image.shape[1], 50))\n",
    "ax.set_yticks(np.arange(0, image.shape[0], 50))\n",
    "plt.imshow(image)\n",
    "plt.grid(True)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Set perspective transform and un-transform matrices\n",
    "img_size = (image.shape[1], image.shape[0])\n",
    "\n",
    "# four source coordinates\n",
    "src = np.float32([p1,p2,p3,p4])\n",
    "\n",
    "# four desired coordinates\n",
    "dst = np.float32([dp1,dp2,dp3,dp4])\n",
    "\n",
    "# compute the perspective transform, M\n",
    "M = cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "# compute the inverse perspective transform, Minv\n",
    "Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "# Define a function that does the actual warping (pass in M), and unwarping (pass in Minv)\n",
    "def warp(img, M):\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    # create warped image, using linear interpolation\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)    \n",
    "    return warped\n",
    "\n",
    "# As a test, warp the image, show the grid\n",
    "image = np.copy(img)\n",
    "# draw source lines on the image in red\n",
    "draw_lines_on_image(image, src_lines, color=[255, 0, 0], thickness=2)\n",
    "# now warp it\n",
    "warped = warp(image,M)\n",
    "# draw destination lines on the warped image in green\n",
    "draw_lines_on_image(warped, dst_lines, color=[0, 255, 0], thickness=2)\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = fig.gca()\n",
    "ax.set_xticks(np.arange(0, warped.shape[1], 50))\n",
    "ax.set_yticks(np.arange(0, warped.shape[0], 50))\n",
    "plt.imshow(warped)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Detect lane lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sliding Windows Search by applying a convolution\n",
    "# -------------------------------------------------\n",
    "# This sliding window method applies a convolution, which maximizes the number of \"hot\" \n",
    "# pixels in each window. A convolution is the summation of the product of two separate \n",
    "# signals, in our case the window template and the vertical slice of the pixel image.\n",
    "#\n",
    "# We slide a window template across the image from left to right and any overlapping \n",
    "# values are summed together, creating the convolved signal. The peak of the convolved \n",
    "# signal is where there was the highest overlap of pixels and the most likely position \n",
    "# for the lane marker.\n",
    "\n",
    "WINDOW_WIDTH = 80 \n",
    "WINDOW_HEIGHT = 40\n",
    "MARGIN = 80 # How much to slide left and right for searching\n",
    "\n",
    "CROP_LEFT = 20\n",
    "CROP_RIGHT = 1260\n",
    "CROP_BOT = 710\n",
    "CROP_TOP = 0\n",
    "\n",
    "MIN_CONVSIGNAL = 100  # if window of convolution has signal lower than this value, it will be rejected\n",
    "#MIN_CONVSIGNAL = 25  # if window of convolution has signal lower than this value, it will be rejected\n",
    "\n",
    "MAX_REJECTED = 100 # if this number of windows are rejected in sequence, reject all following windows\n",
    "                   # this avoids picking up wrong blobs in strongly curved lanes\n",
    "\n",
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "def find_window_centroids(image, verbose=False):\n",
    "    \n",
    "    # window_height must be equal to hood section we skip !\n",
    "    window_width = WINDOW_WIDTH\n",
    "    window_height = WINDOW_HEIGHT\n",
    "    margin = MARGIN\n",
    "    \n",
    "    window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "    window = np.ones(window_width) # Create our window template that we will use for convolutions\n",
    "    \n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "    \n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "    l_sum = np.sum(image[int(3*image.shape[0]/4):,:int(image.shape[1]/2)], axis=0)\n",
    "    l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "    r_sum = np.sum(image[int(3*image.shape[0]/4):,int(image.shape[1]/2):], axis=0)\n",
    "    r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(image.shape[1]/2)\n",
    "    \n",
    "    if verbose:\n",
    "        print('initial l_center, r_center = {0}, {1}'.format(l_center, r_center))\n",
    "    \n",
    "    midpoint = int(image.shape[1]/2)\n",
    "    if l_center == 0:\n",
    "        l_center = 0.5*midpoint\n",
    "    if r_center == 0:\n",
    "        r_center = midpoint + 0.5*midpoint\n",
    "    \n",
    "    if verbose:\n",
    "        print('initial midpoint, l_center, r_center = {0}, {1} {2}'.format(midpoint,l_center, r_center))\n",
    "    \n",
    "    ## Take a histogram of the bottom half of the image\n",
    "    #histogram = np.sum(image[image.shape[0]/2:,:], axis=0)\n",
    "    ## Find the peak of the left and right halves of the histogram\n",
    "    ## Protect it from l_center = 0\n",
    "    #midpoint = np.int(histogram.shape[0]/2)\n",
    "    #l_center = np.argmax(histogram[:midpoint])\n",
    "    #if l_center == 0:\n",
    "    #    l_center = 0.5*midpoint\n",
    "    #r_center = np.argmax(histogram[midpoint:])\n",
    "    #if r_center == 0:\n",
    "    #    r_center = 0.5*midpoint\n",
    "    #r_center = midpoint + r_center\n",
    "    \n",
    "    \n",
    "    # Add what we found for the bottom level\n",
    "    window_centroids.append((l_center,r_center))\n",
    "    \n",
    "    # Keep track if we have found an accepted window\n",
    "    found_accepted_left = False\n",
    "    found_accepted_right = False\n",
    "    \n",
    "    # Keep track of number of rejected windows in sequence\n",
    "    l_num_rejected_sequence = 0\n",
    "    r_num_rejected_sequence = 0\n",
    "    \n",
    "    # Go through each level, including 1st level, looking for max pixel locations\n",
    "    # We go through the bottom level too, to still allow rejection of the window due to to weak a convsignal..\n",
    "    for level in range(0,(int)(image.shape[0]/window_height)):\n",
    "        # convolve the window into the vertical slice of the image\n",
    "        image_layer = np.sum(image[int(image.shape[0]-(level+1)*window_height):int(image.shape[0]-level*window_height),\n",
    "                                   :], axis=0)\n",
    "        conv_signal = np.convolve(window, image_layer)\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        # Use window_width/2 as offset because convolution signal reference is at right side of \n",
    "        # window, not center of window\n",
    "        offset = window_width/2\n",
    "        l_min_index = int(max(l_center+offset-margin, 0.5*window_width ))\n",
    "        l_max_index = int(min(l_center+offset+margin, image.shape[1] - 0.5*window_width))\n",
    "        l_convsignal = np.max(conv_signal[l_min_index:l_max_index])\n",
    "        l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "        \n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center+offset-margin,0.5*window_width ))\n",
    "        r_max_index = int(min(r_center+offset+margin,image.shape[1] - 0.5*window_width))\n",
    "        r_convsignal = np.max(conv_signal[r_min_index:r_max_index])\n",
    "        r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "        \n",
    "        \n",
    "        # If best centroid has too low a convsignal, reject it.\n",
    "        # Mark this by setting it to negative value of center of previous layer\n",
    "        # Also, reject any window once certain number have been rejected in sequence\n",
    "        if l_num_rejected_sequence >= MAX_REJECTED or l_convsignal < MIN_CONVSIGNAL:\n",
    "            l_center = -abs(window_centroids[-1][0])\n",
    "            l_num_rejected_sequence += 1\n",
    "        else:\n",
    "            l_num_rejected_sequence = 0\n",
    "            \n",
    "        if r_num_rejected_sequence >= MAX_REJECTED or r_convsignal < MIN_CONVSIGNAL:\n",
    "            r_center = -abs(window_centroids[-1][1])\n",
    "            r_num_rejected_sequence += 1\n",
    "        else:\n",
    "            r_num_rejected_sequence = 0\n",
    "            \n",
    "        if level==0:\n",
    "            # set it again, perhaps now in rejected status.\n",
    "            window_centroids[0] = (l_center, r_center)\n",
    "        else:\n",
    "            # Add what we found for that layer\n",
    "            window_centroids.append((l_center,r_center))\n",
    "        if verbose:\n",
    "            print ('level, l_center, r_center, l_convsignal, r_convsignal = {0}, {1}, {2}, {3}, {4}'.format(level, l_center, r_center, l_convsignal, r_convsignal) )\n",
    "        \n",
    "        # if this is our first accepted window, correct the ones below to this location\n",
    "        if found_accepted_left == False and l_center > 0:\n",
    "            found_accepted_left = True\n",
    "            for ii in range(len(window_centroids)-1):\n",
    "                window_centroids[ii] = (-l_center, window_centroids[ii][1])\n",
    "        if found_accepted_right == False and r_center > 0:\n",
    "            found_accepted_right = True\n",
    "            for ii in range(len(window_centroids)-1):\n",
    "                window_centroids[ii] = (window_centroids[ii][0], -r_center)\n",
    "    \n",
    "        # reset it to positive values for next level\n",
    "        l_center = abs(l_center)\n",
    "        r_center = abs(r_center)\n",
    "    \n",
    "    return window_centroids\n",
    "\n",
    "# function to extract left and right lane pixel indices inside the windows\n",
    "def fit_polynomials_trough_pixels_in_lane_windows(binary_img_in, \n",
    "                          window_centroids, window_width, window_height,\n",
    "                          visualize=True,verbose=False):\n",
    "    \n",
    "    binary_img = np.copy(binary_img_in)\n",
    "    \n",
    "    # If requested, create & return a new image that visualizes the process\n",
    "    out_img = None\n",
    "    if visualize:\n",
    "        out_img = binary_to_rgb(binary_img)\n",
    "    \n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    dw = 0.5*window_width\n",
    "    dh = 0.5*window_height \n",
    "    yc = binary_img.shape[0]-dh   # y at center of window\n",
    "    \n",
    "    #NO...! Activate one pixel at the center of each rejected window\n",
    "    # This gives mostly a better lane fit in case of lots of rejected windows\n",
    "    activate_center_of_rejected_windows=False\n",
    "    if activate_center_of_rejected_windows:\n",
    "        for centroids in window_centroids: # levels of windows\n",
    "            # x at center of left & right windows at each level\n",
    "            xc_left  = centroids[0]\n",
    "            xc_right = centroids[1]\n",
    "            #print ('xc_left, xc_right, yc={0}, {1}, {2}'.format(xc_left, xc_right, yc))\n",
    "\n",
    "            if xc_left < 0:\n",
    "                binary_img[int(yc), int(abs(xc_left))] = 1\n",
    "            if xc_right < 0:\n",
    "                binary_img[int(yc), int(abs(xc_right))] = 1\n",
    "\n",
    "            # shift level of windows up for next level    \n",
    "            yc = yc - window_height\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    lines = []\n",
    "    yc = binary_img.shape[0]-dh   # y at center of window\n",
    "    for centroids in window_centroids: # levels of windows\n",
    "        # x at center of left & right windows at each level\n",
    "        xc_left  = centroids[0]\n",
    "        xc_right = centroids[1]\n",
    "        \n",
    "        # negative values indicated that window was rejected for this level\n",
    "        # -> pixels in these windows are ignored\n",
    "        # -> the windows will be colored in red instead of green\n",
    "        left_accepted, right_accepted = True, True\n",
    "        color_left, color_rigt = (0,255,0), (0,255,0)\n",
    "        if xc_left <= 0:\n",
    "            left_accepted = False\n",
    "            color_left = (255,0,0)\n",
    "        if xc_right < 0:\n",
    "            right_accepted = False\n",
    "            color_rigt = (255,0,0)\n",
    "        \n",
    "        # set it to positive values, so we can draw it...\n",
    "        xc_left = abs(xc_left)\n",
    "        xc_right = abs(xc_right)\n",
    "        \n",
    "        # left & right window corners\n",
    "        win_y_low       = int(yc - dh)\n",
    "        win_y_high      = int(yc + dh)\n",
    "        win_xleft_low   = int(xc_left  - dw)\n",
    "        win_xleft_high  = int(xc_left  + dw)\n",
    "        win_xright_low  = int(xc_right - dw)\n",
    "        win_xright_high = int(xc_right + dw)\n",
    "        # Identify the nonzero pixels in x and y within the window & Append these indices to the lists\n",
    "        #NO...! Include any pixels in the rejected windows too, because logic took care that it is positioned reasonably, and\n",
    "        #       it improves the polynomial detection.\n",
    "        if left_accepted:\n",
    "            good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "            left_lane_inds.append(good_left_inds)\n",
    "        \n",
    "        if right_accepted:\n",
    "            good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "            right_lane_inds.append(good_right_inds)\n",
    "\n",
    "        # draw the windows if requested\n",
    "        if visualize:\n",
    "            cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),color_left,2) \n",
    "            cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),color_rigt,2) \n",
    "\n",
    "        \n",
    "        # shift level of windows up for next level    \n",
    "        yc = yc - window_height    \n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    if len(left_lane_inds) > 0:\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    if len(right_lane_inds) > 0:\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = None\n",
    "    right_fit = None\n",
    "    if len(left_lane_inds) >0:\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    if len(right_lane_inds) > 0:\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    \n",
    "    # ===========================================================\n",
    "    # Calculate lane curvature in meters\n",
    "    left_curverad = None\n",
    "    rigt_curverad = None\n",
    "    \n",
    "    # Generate x and y values for plotting of polynomials\n",
    "    ploty = np.linspace(0, binary_img.shape[0]-1, binary_img.shape[0] )\n",
    "    \n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    # Fit polynomials to x,y in world space & calculate the radius of curvature\n",
    "    if left_fit is not None:\n",
    "        left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "        left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    if right_fit is not None:    \n",
    "        right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "        right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "\n",
    "    if verbose:\n",
    "        print('left curvature, right curvatue = {0}m, {1}m'.format(left_curverad,right_curverad))\n",
    "        \n",
    "    # ===========================================================\n",
    "    # If requested, color lane pixels and draw polynomial on image\n",
    "    if visualize:\n",
    "        # color left-lane pixels red\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        # color right-lane pixels blue\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "        \n",
    "        if left_fit is not None:\n",
    "            left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "        if right_fit is not None:\n",
    "            right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "        # Draw as yellow lines on the out_image\n",
    "        for i in range(len(ploty) - 1):\n",
    "            # left lane\n",
    "            if left_fit is not None:\n",
    "                p1 = (int(left_fitx[i]  ), int(ploty[i]  ))\n",
    "                p2 = (int(left_fitx[i+1]), int(ploty[i+1]))\n",
    "                cv2.line(out_img, p1, p2, [255, 255, 0], 2)\n",
    "            # right lane\n",
    "            if right_fit is not None:\n",
    "                p1 = (int(right_fitx[i]  ), int(ploty[i]  ))\n",
    "                p2 = (int(right_fitx[i+1]), int(ploty[i+1]))\n",
    "                cv2.line(out_img, p1, p2, [255, 255, 0], 2)\n",
    "    \n",
    "    \n",
    "    return left_fit, right_fit, left_curverad, right_curverad, out_img\n",
    "\n",
    "def find_new_fit(warped, left_fit, right_fit,\n",
    "                 visualize=True,verbose=False):    \n",
    "    # Assume you now have a new warped binary image \n",
    "    # from the next frame of video (also called \"warped\")\n",
    "    # It's now much easier to find line pixels!\n",
    "    nonzero = warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    margin = 100\n",
    "    left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] - margin)) & (nonzerox < (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] + margin))) \n",
    "    right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] - margin)) & (nonzerox < (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] + margin)))  \n",
    "        \n",
    "    # Again, extract left and right line pixel positions &\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = None\n",
    "    right_fit = None\n",
    "    if len(left_lane_inds) >0:\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds] \n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    if len(right_lane_inds) > 0:\n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds]\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, warped.shape[0]-1, warped.shape[0] )\n",
    "    if left_fit is not None:\n",
    "        left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    if right_fit is not None:\n",
    "        right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    #\n",
    "    # If requested, create & return a new image that visualizes the process\n",
    "    result_img = None\n",
    "    if visualize:\n",
    "        out_img = binary_to_rgb(warped)\n",
    "        window_img = np.zeros_like(out_img)\n",
    "        # Color in left and right line pixels\n",
    "        if left_fit is not None:\n",
    "            out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        if right_fit is not None:\n",
    "            out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "        # Generate a polygon to illustrate the search window area\n",
    "        # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "        if left_fit is not None:\n",
    "            left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "            left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, ploty])))])\n",
    "            left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "        if right_fit is not None:\n",
    "            right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "            right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, ploty])))])\n",
    "            right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "        # Draw the lane search area onto the warped blank image\n",
    "        if left_fit is not None:\n",
    "            cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "        if right_fit is not None:\n",
    "            cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "        \n",
    "        result_img = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "        \n",
    "        # Draw the fitted polynomial as yellow lines on the result_img\n",
    "        for i in range(len(ploty) - 1):\n",
    "            # left lane\n",
    "            if left_fit is not None:\n",
    "                p1 = (int(left_fitx[i]  ), int(ploty[i]  ))\n",
    "                p2 = (int(left_fitx[i+1]), int(ploty[i+1]))\n",
    "                cv2.line(result_img, p1, p2, [255, 255, 0], 2)\n",
    "            # right lane\n",
    "            if right_fit is not None:\n",
    "                p1 = (int(right_fitx[i]  ), int(ploty[i]  ))\n",
    "                p2 = (int(right_fitx[i+1]), int(ploty[i+1]))\n",
    "                cv2.line(result_img, p1, p2, [255, 255, 0], 2)\n",
    "        \n",
    "    return left_fit, right_fit, result_img\n",
    "    \n",
    "\n",
    "# using techniques described here: http://www.pyimagesearch.com/2016/03/07/transparent-overlays-with-opencv/\n",
    "def draw_lanes_on_original_image(img,left_fit, right_fit,\n",
    "                                 left_curverad=None, right_curverad=None,\n",
    "                                 text1=None):\n",
    "    # create two copies of the original image -- one for\n",
    "    # the overlay and one for the final output image\n",
    "    overlay_img = img.copy()\n",
    "    out_img     = img.copy()\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # prepare left and right lines as sequence of points\n",
    "    # Generate x and y values for plotting of polynomials\n",
    "    ploty = np.linspace(0, img.shape[0]-1, img.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "    src_pts_left  = []\n",
    "    src_pts_right = []\n",
    "\n",
    "    for i in range(len(ploty)):\n",
    "        src_pts_left.append ( [left_fitx[i] , ploty[i] ] )\n",
    "        src_pts_right.append( [right_fitx[i], ploty[i] ] )\n",
    "\n",
    "    # get source point arrays in correct shape for perspectiveTransform\n",
    "    # see: http://answers.opencv.org/question/252/cv2perspectivetransform-with-python/\n",
    "    src_pts_left = np.array(src_pts_left, dtype='float32')\n",
    "    src_pts_right = np.array(src_pts_right, dtype='float32')\n",
    "\n",
    "    src_pts_left  = np.array([src_pts_left])\n",
    "    src_pts_right = np.array([src_pts_right])\n",
    "\n",
    "    # use the inverse perspective transform to get location of these points in\n",
    "    # the original image\n",
    "    dst_pts_left  = cv2.perspectiveTransform(src_pts_left , Minv)   # shape = (1, 720, 2)\n",
    "    dst_pts_right = cv2.perspectiveTransform(src_pts_right, Minv)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # draw a filled polygon in between lanes on overlay copy, in green\n",
    "    # See: http://stackoverflow.com/questions/11270250/what-does-the-python-interface-to-opencv2-fillpoly-want-as-input\n",
    "    polygon = dst_pts_left[0].tolist() + list(reversed(dst_pts_right[0].tolist()))\n",
    "    polygon = np.array(polygon, 'int32')\n",
    "    cv2.fillConvexPoly(overlay_img, polygon, (0, 255, 0), lineType=8, shift=0)\n",
    "\n",
    "    # draw left lane (red) and right lane (blue) on overlay copy\n",
    "    pts_left = np.array(dst_pts_left, 'int32')\n",
    "    pts_left = pts_left.reshape((-1,1,2))\n",
    "    cv2.polylines(overlay_img,[pts_left],False,(255,0,0),thickness=4)\n",
    "    \n",
    "    pts_right = np.array(dst_pts_right, 'int32')\n",
    "    pts_right = pts_right.reshape((-1,1,2))\n",
    "    cv2.polylines(overlay_img,[pts_right],False,(0,0,255),thickness=4)    \n",
    "\n",
    "    # -----------------------------------------------------------------------  \n",
    "    # apply overlay copy to out_img, with transparency\n",
    "    alpha=0.5\n",
    "    cv2.addWeighted(overlay_img, alpha, out_img, 1 - alpha, 0, out_img)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Put Text on out_img\n",
    "\n",
    "    if text1 is not None:\n",
    "        cv2.putText(out_img, text1,(10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 3)\n",
    "\n",
    "    # Put curvature measures as text on overlay\n",
    "\n",
    "    if left_curverad is not None:\n",
    "        msg = 'left curvature = {0}m'.format(left_curverad)\n",
    "        cv2.putText(out_img, msg,(10, 60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 3)\n",
    "        \n",
    "    if right_curverad is not None:\n",
    "        msg = 'right curvature = {0}m'.format(right_curverad)\n",
    "        cv2.putText(out_img, msg,(10, 60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 3)\n",
    "        \n",
    "    # return new image\n",
    "    return out_img\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put it all in one function...\n",
    "def process_image(image, left_fit_prev=None, right_fit_prev=None, \n",
    "                  show_all=False, verbose=False,\n",
    "                  show_lanes=False):\n",
    "    # NOTE: The output you return should be a color image (3 channel) for processing video below\n",
    "    # Returns the final output (image with lanes drawn on)\n",
    "    global frame\n",
    "    frame += 1\n",
    "    \n",
    "    if show_all: \n",
    "        print('Frame : ', str(frame),'-', type(image), 'with dimesions:', image.shape)\n",
    "        show_image(image, 'Original Image')\n",
    "    \n",
    "    # distortion correct the image\n",
    "    image = undistort(image)\n",
    "    if show_all: show_image(image, 'Distortion Corrected Image')\n",
    "    \n",
    "    # threshold the image\n",
    "    color, binary = pipeline(image, show_all=show_all)\n",
    "    if show_all: \n",
    "        show_image(color , 'Thresholded Image (stacked)')\n",
    "        show_image(binary, 'Thresholded Image', cmap='gray')\n",
    "\n",
    "    # warp the image\n",
    "    warped = warp(binary, M)\n",
    "    if show_all: show_image(warped, 'Warped Image', cmap='gray')\n",
    "    \n",
    "    # crop away hood & outer edges, by turning off pixels\n",
    "    cropped = crop_binary_image(warped,crop_left=CROP_LEFT, crop_right=CROP_RIGHT,\n",
    "                                       crop_bot=CROP_BOT, crop_top=CROP_TOP)\n",
    "    if show_all: show_image(cropped, 'Cropped', cmap='gray')        \n",
    "\n",
    "    # Find the windows - from scratch\n",
    "    if left_fit_prev is None:\n",
    "        window_centroids = find_window_centroids(cropped,verbose=verbose)\n",
    "\n",
    "        left_fit, right_fit, left_curverad, right_curverad, viz_warped = fit_polynomials_trough_pixels_in_lane_windows(\n",
    "                                           cropped, \n",
    "                                           window_centroids, WINDOW_WIDTH, WINDOW_HEIGHT,\n",
    "                                           visualize=True,verbose=verbose)\n",
    "    else:\n",
    "        # Use this during video processing !\n",
    "        # Find the windows - in region around left_fit, right_fit of previous image\n",
    "        left_fit, right_fit, viz_warped = find_new_fit(cropped, \n",
    "                                                       left_fit_prev, right_fit_prev,\n",
    "                                                       visualize=True,verbose=verbose)\n",
    "    \n",
    "    if show_all or show_lanes: show_image(viz_warped, 'Warped Image with Lane Windows')\n",
    "    \n",
    "    # Draw the lanes on original image\n",
    "    if left_fit is not None and right_fit is not None:\n",
    "        final_image = draw_lanes_on_original_image(image,left_fit, right_fit,\n",
    "                                                   left_curverad, right_curverad,\n",
    "                                                   text1=\"frame {0}\".format(frame))\n",
    "    else:\n",
    "        final_image = image\n",
    "        \n",
    "    if show_all or show_lanes: show_image(final_image, 'Original Image with Lanes')\n",
    "    \n",
    "    return final_image, left_fit, right_fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Testing the lane finding using process_image function')\n",
    "\n",
    "# Make a list of test images\n",
    "\n",
    "use_prev_fit = False\n",
    "file_dir = \"test_images\"\n",
    "file_dir_out = \"test_images_output\"\n",
    "files = os.listdir(file_dir)\n",
    "files = ['project_video_output0611.jpg']\n",
    "#files = ['challenge_video_output0239.jpg']\n",
    "#files = ['challenge_video_output0001.jpg']\n",
    "#files = ['challenge_video_output0025.jpg']\n",
    "\n",
    "#======================================================\n",
    "# Project Video\n",
    "#use_prev_fit = True\n",
    "#file_dir = \"test_images_project_video\"\n",
    "#file_dir_out = \"test_images_project_video_output\"\n",
    "#files = os.listdir(file_dir)\n",
    "\n",
    "#use_prev_fit=True\n",
    "#files = ['project_video_output0001.jpg',\n",
    "#         'project_video_output0002.jpg',\n",
    "#         'project_video_output0003.jpg',\n",
    "#         'project_video_output0004.jpg',\n",
    "#         'project_video_output0005.jpg',\n",
    "#         'project_video_output0006.jpg']\n",
    "\n",
    "#use_prev_fit=False\n",
    "#files = ['project_video_output0626.jpg',\n",
    "#         'project_video_output0547.jpg',\n",
    "#         'project_video_output0577.jpg',\n",
    "#         'project_video_output0994.jpg',\n",
    "#         'project_video_output1003.jpg',\n",
    "#         'project_video_output1047.jpg']\n",
    "\n",
    "#======================================================\n",
    "# Challenge Video\n",
    "#use_prev_fit = False\n",
    "#file_dir = \"test_images_challenge_video\"\n",
    "#file_dir_out = \"test_images_challenge_video_output\"\n",
    "#files = os.listdir(file_dir)\n",
    "\n",
    "#use_prev_fit=True\n",
    "#files = ['challenge_video_output0001.jpg',\n",
    "#         'challenge_video_output0002.jpg',\n",
    "#         'challenge_video_output0003.jpg',\n",
    "#         'challenge_video_output0004.jpg',\n",
    "#         'challenge_video_output0005.jpg',\n",
    "#         'challenge_video_output0006.jpg']\n",
    "#files = ['challenge_video_output0484.jpg']\n",
    "\n",
    "#use_prev_fit=False\n",
    "#files = ['challenge_video_output0239.jpg']\n",
    "\n",
    "\n",
    "# test process_image function...\n",
    "frame=0\n",
    "left_fit_prev=None\n",
    "right_fit_prev=None\n",
    "show_all   = False\n",
    "verbose    = False\n",
    "show_lanes = True\n",
    "show_final = False\n",
    "for file in tqdm(files):\n",
    "    file_in=file_dir+\"/\"+file\n",
    "    file_out=file_dir_out+\"/\"+file\n",
    "    file_out2=file_dir_out+\"/unwarped_with_lanes_\"+file\n",
    "    \n",
    "    image = mpimg.imread(file_in)\n",
    "    \n",
    "    if show_lanes:\n",
    "        show_image(image,title=file)\n",
    "    \n",
    "    final_image, left_fit, right_fit, left_curverad, right_curverad = process_image(\n",
    "                                                    image, \n",
    "                                                    left_fit_prev, right_fit_prev,\n",
    "                                                    show_all=show_all, verbose=verbose,\n",
    "                                                    show_lanes=show_lanes)\n",
    "    plt.imsave(file_out,final_image)\n",
    "    \n",
    "    if use_prev_fit:\n",
    "        left_fit_prev = left_fit\n",
    "        right_fit_prev = right_fit\n",
    "    \n",
    "    if show_final:\n",
    "        show_image(final_image,title=file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Videos\n",
    "\n",
    "We can test our solution on three provided videos:\n",
    "- project_video.mp4\n",
    "- challenge_video.mp4\n",
    "- harder_challenge_video.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame = 0\n",
    "project_video_output = 'project_video_output.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "project_video_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time project_video_clip.write_videofile(project_video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(project_video_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ... old stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reading in an image\n",
    "image = mpimg.imread('test_images/straight_lines1.jpg')\n",
    "#printing out some stats and plotting\n",
    "print('This image is:', type(image), 'with dimesions:', image.shape)\n",
    "plt.imshow(image)  # if you wanted to show a single color channel image called 'gray', for example, call as plt.imshow(gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some helper functions to help get you started. They should look familiar from the lesson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n",
    "    \"\"\"\n",
    "    NOTE: this is the function you might want to use as a starting point once you want to \n",
    "    average/extrapolate the line segments you detect to map out the full\n",
    "    extent of the lane (going from the result shown in raw-lines-example.mp4\n",
    "    to that shown in P1_example.mp4).  \n",
    "    \n",
    "    Think about things like separating line segments by their \n",
    "    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n",
    "    line vs. the right line.  Then, you can average the position of each of \n",
    "    the lines and extrapolate to the top and bottom of the lane.\n",
    "    \n",
    "    This function draws `lines` with `color` and `thickness`.    \n",
    "    Lines are drawn on the image inplace (mutates the image).\n",
    "    If you want to make the lines semi-transparent, think about combining\n",
    "    this function with the weighted_img() function below\n",
    "    \"\"\"\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    `img` should be the output of a Canny transform.\n",
    "        \n",
    "    Returns an image with hough lines drawn.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    draw_lines(line_img, lines)\n",
    "    return line_img\n",
    "\n",
    "# Python 3 has support for cool math symbols.\n",
    "\n",
    "def weighted_img(img, initial_img, =0.8, =1., =0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img *  + img *  + \n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, , img, , )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Images\n",
    "\n",
    "Now you should build your pipeline to work on the images in the directory \"test_images\"  \n",
    "**You should make sure your pipeline works well on these images before you try the videos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir(\"test_images/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run your solution on all test_images and make copies into the test_images directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Build your pipeline that will draw lane lines on the test_images\n",
    "# then save them to the test_images directory.\n",
    "def draw_lanes_on_image(file_in, file_out, show_all=True, show_final=True):\n",
    "    ####################################################################\n",
    "    #reading in an image\n",
    "    image = mpimg.imread(file_in)\n",
    "    print (\"******************************************************************\")\n",
    "    print('Processing image from file: '+file_in)\n",
    "    combo = process_image(image, show_all, show_final)\n",
    "    \n",
    "    plt.imshow(combo)\n",
    "    plt.savefig(file_out)\n",
    "    print('Processed image saved as: ' + file_out)\n",
    "   \n",
    "def process_image(image, show_all=False, show_final=False):\n",
    "    # NOTE: The output you return should be a color image (3 channel) for processing video below\n",
    "    # TODO: put your pipeline here,\n",
    "    # you should return the final output (image with lines are drawn on lanes)\n",
    "    global frame\n",
    "    frame += 1\n",
    "    \n",
    "    print('Frame : ', str(frame),'-', type(image), 'with dimesions:', image.shape)\n",
    "    if show_all: show_image(image, 'Original Image')\n",
    "    \n",
    "    ####################################################################\n",
    "    # Make it gray scale\n",
    "    gray = grayscale(image)\n",
    "    if show_all: show_image(gray, 'Grayscale Image', cmap='gray')\n",
    "\n",
    "    ####################################################################\n",
    "    # Next we'll create a masked image with only region of interest (roi)\n",
    "    roi_vertices = calculate_region_of_interest(image, show_all)   \n",
    "    masked_gray = region_of_interest(gray, roi_vertices)\n",
    "    if show_all: show_image(masked_gray, 'Grayscale Region of Interest', cmap='Greys_r')\n",
    "    \n",
    "    ####################################################################\n",
    "    # \"Equalize\" the dark gray levels in region of interest, so the lighter\n",
    "    # lines that represent the lanes are more pronounced, and the canny\n",
    "    # edge detection will be more precise.\n",
    "    # \n",
    "    # I did this to get challenge_frame_110 to work properly:\n",
    "    # (-) The road is concrete (light)\n",
    "    # (-) There are black tire skid marks that are detected by canny and\n",
    "    #      mess it all up without this step\n",
    "    #\n",
    "    equalize_gray_level_in_roi(masked_gray)\n",
    "    if show_all: show_image(masked_gray, 'Equalized gray level', cmap='Greys_r')\n",
    "    \n",
    "    ####################################################################\n",
    "    edges = find_edges_with_canny(masked_gray)\n",
    "    if show_all: show_image(edges, 'Canny Edges', cmap='Greys_r')\n",
    "    \n",
    "    ####################################################################\n",
    "    # Find the lanes, using hough transform\n",
    "    #\n",
    "    # Define the Hough transform parameters\n",
    "    rho             = 2          # distance resolution in pixels of the Hough grid\n",
    "    theta           = np.pi/180  # angular resolution in radians of the Hough grid\n",
    "    threshold       = 15         # minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_len    = 10         # minimum number of pixels making up a line\n",
    "    max_line_gap    = 5          # maximum gap in pixels between connectable line segments\n",
    "    \n",
    "    lines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    if lines is None :\n",
    "        print ('ERROR: No hough lines found')\n",
    "        return image\n",
    "    \n",
    "    if show_all:\n",
    "        line_img = np.zeros((edges.shape[0], edges.shape[1], 3), \n",
    "                            dtype=np.uint8)\n",
    "        draw_lines(line_img, lines)\n",
    "        combo = weighted_img(line_img, image, =0.8, =1., =0.)\n",
    "        show_image(combo, 'Hough Lines on Image')\n",
    "\n",
    "    ####################################################################\n",
    "    # Draw the lanes on the original image    \n",
    "    line_img = np.zeros((image.shape[0], image.shape[1], 3), \n",
    "                        dtype=np.uint8)\n",
    "    create_lanes_from_hough_lines(line_img, roi_vertices, lines, thickness=10)\n",
    "    combo = weighted_img(line_img, image, =0.8, =1., =0.)\n",
    "    if show_all or show_final: show_image(combo, 'Final Image with Lanes')\n",
    "\n",
    "    return combo\n",
    "\n",
    "def show_image(image, title, cmap=None ):\n",
    "    plt.title(title)\n",
    "    if cmap:\n",
    "        plt.imshow(image, cmap=cmap) # if you wanted to show a single color channel image called 'gray', for example, call as plt.imshow(gray, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(image)  \n",
    "    plt.show()\n",
    "    \n",
    "def calculate_region_of_interest(image, show_all):\n",
    "    imshape = image.shape\n",
    "    \n",
    "    # height of roi region\n",
    "    if roi_height == 0:\n",
    "        height = int( 0.4*imshape[0] )\n",
    "    else:\n",
    "        height = roi_height\n",
    "        \n",
    "    # bottom width of roi region\n",
    "    if roi_width_bot == 0:\n",
    "        width_bot = int( 0.80 * imshape[1] )\n",
    "    else:\n",
    "        width_bot = roi_width_bot\n",
    "        \n",
    "    # top width of roi region\n",
    "    if roi_width_top == 0:\n",
    "        width_top = int( 0.20 * imshape[1] )\n",
    "    else:\n",
    "        width_top = roi_width_top\n",
    "      \n",
    "    x_bot_min = roi_shift_bot + int( 0.5 * (imshape[1] - width_bot) )\n",
    "    x_bot_max = roi_shift_bot + int( 0.5 * (imshape[1] + width_bot) )\n",
    "    x_top_min = roi_shift_top + int( 0.5 * (imshape[1] - width_top) )\n",
    "    x_top_max = roi_shift_top + int( 0.5 * (imshape[1] + width_top) )\n",
    "    \n",
    "    # eliminate car hood from picture\n",
    "    y_bot = imshape[0] - hood_size\n",
    "    y_top = y_bot - height\n",
    "    \n",
    "    roi_vertices = np.array([[(x_bot_min,y_bot),\n",
    "                              (x_top_min, y_top), \n",
    "                              (x_top_max, y_top), \n",
    "                              (x_bot_max,y_bot)]], dtype=np.int32)\n",
    "    \n",
    "    if show_all:\n",
    "        plt.title('Region of interest on Image. Wb,Wt,H,Sb,St,Sv='+\n",
    "                  str(x_bot_max-x_bot_min)+','+\n",
    "                  str(x_top_max-x_top_min)+','+\n",
    "                  str(y_top-y_bot)+','+\n",
    "                  str(roi_shift_bot)+','+\n",
    "                  str(roi_shift_top)+','+\n",
    "                  str(hood_size))\n",
    "        line_img = np.zeros((image.shape[0], image.shape[1], 3), \n",
    "                            dtype=np.uint8)\n",
    "        p=roi_vertices[0]\n",
    "        roi_lines = np.array([[[ p[0, 0], p[0, 1], p[1, 0], p[1, 1] ]],\n",
    "                              [[ p[1, 0], p[1, 1], p[2, 0], p[2, 1] ]],\n",
    "                              [[ p[2, 0], p[2, 1], p[3, 0], p[3, 1] ]],\n",
    "                              [[ p[3, 0], p[3, 1], p[0, 0], p[0, 1] ]]], \n",
    "                             dtype=np.int32)\n",
    "        draw_lines(line_img, roi_lines)\n",
    "        combo = weighted_img(line_img, image, =0.8, =1., =0.)\n",
    "        plt.imshow(combo)\n",
    "        plt.show()\n",
    "        \n",
    "    return roi_vertices\n",
    "\n",
    "def equalize_gray_level_in_roi(masked_gray): \n",
    "    # calculate the average gray level in the region of interest   \n",
    "    gray_average = int( np.average(masked_gray[np.nonzero(masked_gray)]) )\n",
    "    \n",
    "    # assign this average gray level to all regions that are darker than this\n",
    "    # average value\n",
    "    masked_gray[ masked_gray < gray_average ] = gray_average\n",
    "\n",
    "def find_edges_with_canny(masked_gray):    \n",
    "    # See: http://homepages.inf.ed.ac.uk/rbf/HIPR2/canny.htm\n",
    "    #\n",
    "    # Define a kernel size for Gaussian smoothing / blurring\n",
    "    # Note: this step is optional as cv2.Canny() applies a 5x5 Gaussian internally\n",
    "    kernel_size = 5\n",
    "    gaussian_blur(masked_gray, kernel_size)\n",
    "    \n",
    "    # Define parameters for Canny and run it\n",
    "    # thresholds between 0 and 255\n",
    "    low_threshold = 50\n",
    "    high_threshold = 150\n",
    "    edges = canny(masked_gray, low_threshold, high_threshold)\n",
    "    \n",
    "    return edges \n",
    "   \n",
    "def create_lanes_from_hough_lines(img, roi_vertices, lines, color=[255, 0, 0], thickness=None):\n",
    "    \"\"\"\n",
    "    This routine attempts to extract the left and right lane from the provided\n",
    "    hough lines, and draws them on the image.\n",
    "    \n",
    "    The method used is as follows:\n",
    "    (-) each line is extrapolated to the top and bottom of the region of interest\n",
    "    (-) it will be discarded if the intersection at the top and bottom is outside\n",
    "        the region of interest.\n",
    "    (-) the remaining lines are assigned to the left lane if the angle is negative,\n",
    "        else they are assigned to the right lane\n",
    "    (-) the locations of the line intersections with the top and bottom of \n",
    "        the region of interest are averaged to provide the lane end points\n",
    "        \n",
    "    NOTE: This logic ONLY works if the lanes within the region of interest are\n",
    "          straight.\n",
    "    \"\"\"\n",
    "    #\n",
    "    # extrapolate start and end points for each line to boundary of region of interest\n",
    "    #\n",
    "    y_top_roi    = roi_vertices[0, 1, 1]\n",
    "    y_bot_roi    = roi_vertices[0, 0, 1]\n",
    "    \n",
    "    x_top_roi_min   = roi_vertices[0, 1, 0]\n",
    "    x_top_roi_max   = roi_vertices[0, 2, 0]\n",
    "    x_bot_roi_min   = roi_vertices[0, 0, 0]\n",
    "    x_bot_roi_max   = roi_vertices[0, 3, 0]\n",
    "    \n",
    "    x_tops_left   = []\n",
    "    x_bots_left   = []\n",
    "    \n",
    "    x_tops_right   = []\n",
    "    x_bots_right   = []\n",
    "    \n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "        \n",
    "            # fit a line (y=Ax+B) through this line\n",
    "            # np.polyfit() returns the coefficients [A, B] of the fit\n",
    "            fit_line = np.polyfit((x1, x2), (y1, y2), 1)\n",
    "            A, B = fit_line\n",
    "            \n",
    "            # skip lines with very small slope (A)\n",
    "            if abs(A) < 0.1:\n",
    "                continue\n",
    "            \n",
    "            # calculate intersection with top & bottom boundary of region of interest\n",
    "            x_top = (y_top_roi - B) / A\n",
    "            x_bot = (y_bot_roi - B) / A\n",
    "            \n",
    "            # skip this line if either the top or bottom points are outside region of interest\n",
    "            if x_top < x_top_roi_min or x_top > x_top_roi_max:\n",
    "                continue\n",
    "            if x_bot < x_bot_roi_min or x_bot > x_bot_roi_max:\n",
    "                continue\n",
    "            \n",
    "            # negative angle line --> left lane\n",
    "            if A < 0:\n",
    "                x_tops_left.append(x_top)\n",
    "                x_bots_left.append(x_bot)\n",
    "            else:\n",
    "                x_tops_right.append(x_top)\n",
    "                x_bots_right.append(x_bot)\n",
    "    \n",
    "    # if we didn't find a top or bottom, then skip this image\n",
    "    if (len(x_tops_left) == 0 or\n",
    "        len(x_bots_left) == 0 or\n",
    "        len(x_tops_right) == 0 or\n",
    "        len(x_bots_right) == 0 ):\n",
    "        print ('Not all end points found --> skipping this image !!')\n",
    "        return\n",
    "    \n",
    "    x_top_left_average  = int( np.array(x_tops_left).mean()  )\n",
    "    x_top_right_average = int( np.array(x_tops_right).mean() )\n",
    "    x_bot_left_average  = int( np.array(x_bots_left).mean()  )\n",
    "    x_bot_right_average = int( np.array(x_bots_right).mean() )\n",
    "    \n",
    "    x_top_left_width  = max(x_tops_left)  - min(x_tops_left)\n",
    "    x_top_right_width = max(x_tops_right) - min(x_tops_right)\n",
    "    x_bot_left_width  = max(x_bots_left)  - min(x_bots_left)\n",
    "    x_bot_right_width = max(x_bots_right) - min(x_bots_right)\n",
    "    \n",
    "    if thickness == None:\n",
    "        thickness = int( 0.5*sum( [x_top_left_width ,\n",
    "                                   x_top_right_width,\n",
    "                                   x_bot_left_width ,\n",
    "                                   x_bot_right_width] ) / 4 )\n",
    "    \n",
    "    # plot left lane                 \n",
    "    cv2.line(img, (x_bot_left_average, y_bot_roi), \n",
    "                  (x_top_left_average, y_top_roi), color, thickness)\n",
    "    \n",
    "    # plot right lane                 \n",
    "    cv2.line(img, (x_bot_right_average, y_bot_roi), \n",
    "                  (x_top_right_average, y_top_roi), color, thickness)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Import everything needed to edit/save/watch video clips\n",
    "    from moviepy.editor import VideoFileClip\n",
    "    from IPython.display import HTML\n",
    "    \n",
    "    ################################################################\n",
    "    # process some test files\n",
    "    #\n",
    "    file_dir = \"test_images\"\n",
    "    file_dir_out = \"test_images_output\"\n",
    "    files = os.listdir(file_dir)\n",
    " \n",
    "    frame = 0\n",
    "    hood_size=45\n",
    "    roi_height=200\n",
    "    roi_width_bot=1200\n",
    "    roi_width_top=500\n",
    "    roi_shift_bot=0\n",
    "    roi_shift_top=0\n",
    "    for file in files:\n",
    "        frame = 0\n",
    "        draw_lanes_on_image(file_in=file_dir+\"/\"+file, \n",
    "                            file_out=file_dir_out+\"/with_lanes_\"+file,\n",
    "                            show_all=False, show_final=True)\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Videos\n",
    "\n",
    "You know what's cooler than drawing lanes over images? Drawing lanes over video!\n",
    "\n",
    "We can test our solution on three provided videos:\n",
    "\n",
    "`project_video.mp4`\n",
    "\n",
    "`challenge_video.mp4`\n",
    "\n",
    "`harder_challenge_video.mp4`\n",
    "\n",
    "**Note: if you get an `import error` when you run the next cell, try changing your kernel (select the Kernel menu above --> Change Kernel).  Still have problems?  Try relaunching Jupyter Notebook from the terminal prompt. Also, check out [this forum post](https://carnd-forums.udacity.com/questions/22677062/answers/22677109) for more troubleshooting tips.**\n",
    "\n",
    "**If you get an error that looks like this:**\n",
    "```\n",
    "NeedDownloadError: Need ffmpeg exe. \n",
    "You can download it by calling: \n",
    "imageio.plugins.ffmpeg.download()\n",
    "```\n",
    "**Follow the instructions in the error message and check out [this forum post](https://carnd-forums.udacity.com/display/CAR/questions/26218840/import-videofileclip-error) for more troubleshooting tips across operating systems.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def process_image is coded up above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the project_video.mp4 first ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = 0\n",
    "hood_size=45\n",
    "roi_height=200\n",
    "roi_width_bot=1200\n",
    "roi_width_top=500\n",
    "roi_shift_bot=0\n",
    "roi_shift_top=0\n",
    "project_video_output = 'project_video_output.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "project_video_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time project_video_clip.write_videofile(project_video_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the video inline, or if you prefer find the video in your filesystem (should be in the same directory) and play it in your video player of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(project_video_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, if you were successful you probably have the Hough line segments drawn onto the road, but what about identifying the full extent of the lane and marking it clearly as in the example video (P1_example.mp4)?  Think about defining a line to run the full length of the visible lane based on the line segments you identified with the Hough Transform.  Modify your draw_lines function accordingly and try re-running your pipeline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the one with the solid yellow lane on the left. This one's more tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = 0\n",
    "hood_size=0\n",
    "roi_height=200\n",
    "roi_width_bot=810\n",
    "roi_width_top=150\n",
    "roi_shift_bot=45\n",
    "roi_shift_top=15\n",
    "yellow_output = 'yellow.mp4'\n",
    "clip2 = VideoFileClip('solidYellowLeft.mp4')\n",
    "yellow_clip = clip2.fl_image(process_image)\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflections\n",
    "\n",
    "Congratulations on finding the lane lines!  As the final step in this project, we would like you to share your thoughts on your lane finding pipeline... specifically, how could you imagine making your algorithm better / more robust?  Where will your current algorithm be likely to fail?\n",
    "\n",
    "Please add your thoughts below,  and if you're up for making your pipeline more robust, be sure to scroll down and check out the optional challenge video below!\n",
    "\n",
    "---\n",
    "My pipeline was largely created based on frame 110 of the challenge video:\n",
    "<figure>\n",
    " <img src=\"challenge_frame_110_image-0.jpg\" width=\"380\" alt=\"Original\" />\n",
    "</figure>\n",
    "\n",
    "The steps take are best explained by this sequence of pictures:\n",
    "\n",
    "<figure>\n",
    " <img src=\"challenge_frame_110_image-1.jpg\" width=\"380\" alt=\"Grayscale\" />\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img src=\"challenge_frame_110_image-2.jpg\" width=\"380\" alt=\"ROI\" />\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img src=\"challenge_frame_110_image-3.jpg\" width=\"380\" alt=\"Gray Scale ROI\" />\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img src=\"challenge_frame_110_image-4.jpg\" width=\"380\" alt=\"Equalized\" />\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img src=\"challenge_frame_110_image-5.jpg\" width=\"380\" alt=\"Canny Edges\" />\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img src=\"challenge_frame_110_image-6.jpg\" width=\"380\" alt=\"Hough Lines\" />\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img src=\"challenge_frame_110_image-7.jpg\" width=\"380\" alt=\"Final Image\" />\n",
    "</figure>\n",
    " \n",
    "Some key decisions I made along the way:\n",
    "\n",
    "1. I parameterized the inputs for the region of interest (roi):\n",
    "        hood_size\n",
    "        roi_height\n",
    "        roi_width_bot\n",
    "        roi_width_top\n",
    "        roi_shift_bot\n",
    "        roi_shift_top\n",
    "    \n",
    "    My logic  strongly depends on having a well defined region of interest.\n",
    "\n",
    "2. I 'equalize' the grey level in the region of interest.\n",
    "...This greatly improves the visibility of the lanes in the image.\n",
    "...See function: equalize_gray_level_in_roi\n",
    "    \n",
    "3. All images and videos have straight lanes in the region of interest. I make use of this fact, in two ways:\n",
    "..* To select what hough lines belong to the lanes, I extrapolate them to the top and bottom of the region of interest. If they intersect outside of the region of interest then they obviously are not part of the lanes.\n",
    "\n",
    "..* To determine the end points of the lanes, I simply average the intersection points of the selected lines with the top and bottom of the region of interest. The lanes are then drawn with a fixed thickness.\n",
    "\n",
    "## Weaknesses\n",
    "\n",
    "The weakest aspects of my approach are:\n",
    "\n",
    "* The requirement to manually set the region of interest. If the car drifts, I can see that we lose the lanes very easily.\n",
    "\n",
    "* The 'equalizing' of gray level in the region of interest does not seem so robust. It works ok for all tests, but I can imagine it will fail on other cases.\n",
    "\n",
    "* The logic only works on straight lines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "If you're satisfied with your video outputs it's time to submit!  Submit this ipython notebook for review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional Challenge\n",
    "\n",
    "Try your lane finding pipeline on the video below.  Does it still work?  Can you figure out a way to make it more robust?  If you're up for the challenge, modify your pipeline so it works with this video and submit it along with the rest of your project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = 0\n",
    "hood_size=55\n",
    "roi_height=200\n",
    "roi_width_bot=840\n",
    "roi_width_top=180\n",
    "roi_shift_bot=35\n",
    "roi_shift_top=25\n",
    "challenge_output = 'extra.mp4'\n",
    "clip2 = VideoFileClip('challenge.mp4')\n",
    "challenge_clip = clip2.fl_image(process_image)\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
